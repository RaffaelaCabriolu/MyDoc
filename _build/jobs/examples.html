

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Job script examples &mdash; HPC Physics documentation 0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="SLURM Workload Manager" href="slurm_parameter.html" />
    <link rel="prev" title="Batch system" href="batch.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> HPC Physics documentation
          

          
          </a>

          
            
            
              <div class="version">
                0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../clusters/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../news/news.html">News and notifications</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting help</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../help/contact.html">Contact</a></li>
<li class="toctree-l1"><a class="reference internal" href="../help/staff.html">Support staff and Contact</a></li>
<li class="toctree-l1"><a class="reference internal" href="../help/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../help/faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../help/hpc-qa-sessions.html">Forum, Open Question &amp; Answer Sessions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jobs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dos_and_donts.html">Dos and don’ts</a></li>
<li class="toctree-l1"><a class="reference internal" href="batch.html">Batch system</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Job script examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basic-examples">Basic examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#general-blueprint-for-a-jobscript">General blueprint for a jobscript</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-many-sequential-jobs-in-parallel-using-job-arrays">Running many sequential jobs in parallel using job arrays</a></li>
<li class="toctree-l3"><a class="reference internal" href="#packaging-smaller-parallel-jobs-into-one-large-parallel-job">Packaging smaller parallel jobs into one large parallel job</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-on-how-to-allocate-entire-memory-on-one-node">Example on how to allocate entire memory on one node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-recover-files-before-a-job-times-out">How to recover files before a job times out</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#openmp-and-mpi">OpenMP and MPI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-for-an-openmp-job">Example for an OpenMP job</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-for-a-mpi-job">Example for a MPI job</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-for-a-hybrid-mpi-openmp-job">Example for a hybrid MPI/OpenMP job</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="slurm_parameter.html">SLURM Workload Manager</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="job_management.html">Managing jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring your jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="running_mpi_jobs.html">Running MPI jobs</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">HPC Physics documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Job script examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/jobs/examples.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="job-script-examples">
<span id="id1"></span><h1>Job script examples<a class="headerlink" href="#job-script-examples" title="Permalink to this headline">¶</a></h1>
<section id="basic-examples">
<h2>Basic examples<a class="headerlink" href="#basic-examples" title="Permalink to this headline">¶</a></h2>
<section id="general-blueprint-for-a-jobscript">
<h3>General blueprint for a jobscript<a class="headerlink" href="#general-blueprint-for-a-jobscript" title="Permalink to this headline">¶</a></h3>
<p>You can copy the following example into a file (e.g. run.sh) on the HPC local cluster or in the IDUN system. Comment
the two <code class="docutils literal notranslate"><span class="pre">cp</span></code> commands that are just for illustratory purpose (lines 46 and 55)
and change the SBATCH directives where applicable. You can then run the script
by typing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sbatch run.sh
</pre></div>
</div>
<p>Please note that all values that you define with SBATCH directives are hard
values. When you, for example, ask for 6000 MB of memory (<code class="docutils literal notranslate"><span class="pre">--mem=6000MB</span></code>) and
your job uses more than that, the job will be automatically killed by the manager.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>

<span class="c1">##############################</span>
<span class="c1">#       Job blueprint        #</span>
<span class="c1">##############################</span>

<span class="c1"># Give your job a name, so you can recognize it in the queue overview</span>
<span class="c1">#SBATCH --job-name=example</span>

<span class="c1"># Define, how many nodes you need. Here, we ask for 1 node.</span>
<span class="c1"># Each node has 16 or 20 CPU cores.</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1"># You can further define the number of tasks with --ntasks-per-*</span>
<span class="c1"># See &quot;man sbatch&quot; for details. e.g. --ntasks=4 will ask for 4 cpus.</span>

<span class="c1"># Define, how long the job will run in real time. This is a hard cap meaning</span>
<span class="c1"># that if the job runs longer than what is written here, it will be</span>
<span class="c1"># force-stopped by the server. If you make the expected time too long, it will</span>
<span class="c1"># take longer for the job to start. Here, we say the job will take 5 minutes.</span>
<span class="c1">#              d-hh:mm:ss</span>
<span class="c1">#SBATCH --time=0-00:05:00</span>

<span class="c1"># Define the partition on which the job shall run, if it is omitted, the job</span>
<span class="c1"># will be automatically placed in the normal-partition. </span>
<span class="c1">#SBATCH --partition normal</span>

<span class="c1"># The following allows to require the needed memory.</span>
<span class="c1"># &quot;--mem&quot; defines memory per node and</span>
<span class="c1"># &quot;--mem-per-cpu&quot; defines memory per CPU/core. Choose one of those.</span>
<span class="c1">#SBATCH --mem-per-cpu=1500MB</span>
<span class="c1">##SBATCH --mem=5GB    # every line with a double hash is commented.</span>

<span class="c1"># With the following line you can turn on mail notification. </span>
<span class="c1"># There are many possible self-explaining values:</span>
<span class="c1"># NONE, BEGIN, END, FAIL, ALL (including all aforementioned)</span>
<span class="c1"># For more values, check &quot;man sbatch&quot;</span>
<span class="c1">#SBATCH --mail-type=END,FAIL</span>

<span class="c1"># You may not place any commands before the last SBATCH directive</span>

<span class="c1"># Define and create a unique scratch directory for this job. </span>
<span class="nv">SCRATCH_DIRECTORY</span><span class="o">=</span>/work/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/<span class="si">${</span><span class="nv">SLURM_JOBID</span><span class="si">}</span>.phys.ntnu.no
mkdir -p <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># You can copy everything you need to the scratch directory</span>
<span class="c1"># ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from</span>
cp <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>/myfiles*.txt <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># This is where the actual work is done. In this case, the script only waits.</span>
<span class="c1"># The time command is optional, but it may give you a hint on how long the</span>
<span class="c1"># command worked</span>
<span class="nb">time</span> sleep <span class="m">10</span>
<span class="c1">#sleep 10</span>

<span class="c1"># After the job is done we copy our output back to $SLURM_SUBMIT_DIR</span>
cp <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>/my_output <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>

<span class="c1"># In addition to the copied files, you will also find a file called</span>
<span class="c1"># slurm-1234.out in the submit directory. This file will contain all output that</span>
<span class="c1"># was produced during runtime, i.e. stdout and stderr.</span>

<span class="c1"># After everything is saved to the home directory, delete the work directory to</span>
<span class="c1"># save space on the /work directory</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>
rm -rf <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># Finish the script</span>
<span class="nb">exit</span> <span class="m">0</span>
</pre></div>
</div>
</section>
<section id="running-many-sequential-jobs-in-parallel-using-job-arrays">
<span id="job-arrays"></span><h3>Running many sequential jobs in parallel using job arrays<a class="headerlink" href="#running-many-sequential-jobs-in-parallel-using-job-arrays" title="Permalink to this headline">¶</a></h3>
<p>In this example we wish to run many similar sequential jobs in parallel using job arrays.
We take Python as an example but this does not matter for the job arrays:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="kn">import</span> <span class="nn">time</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;start at &#39;</span> <span class="o">+</span> <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%H:%M:%S&#39;</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sleep for 10 seconds ...&#39;</span><span class="p">)</span>
<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;stop at &#39;</span> <span class="o">+</span> <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%H:%M:%S&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>Save this to a file called “test.py” and try it out:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ python test.py

start at 15:23:48
sleep for 10 seconds ...
stop at 15:23:58
</pre></div>
</div>
<p>Now we wish to run this script 16 times at the same time.
For this we use the following script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>

<span class="c1">#####################</span>
<span class="c1"># job-array example #</span>
<span class="c1">#####################</span>

<span class="c1">#SBATCH --job-name=array_example</span>

<span class="c1"># 16 jobs will run in this array at the same time</span>
<span class="c1">#SBATCH --array=1-16</span>

<span class="c1"># run for five minutes</span>
<span class="c1">#              d-hh:mm:ss</span>
<span class="c1">#SBATCH --time=0-00:05:00</span>

<span class="c1"># 500MB memory per core</span>
<span class="c1"># this is a hard limit</span>
<span class="c1">#SBATCH --mem-per-cpu=500MB</span>

<span class="c1"># define and create a unique scratch directory</span>
<span class="nv">SCRATCH_DIRECTORY</span><span class="o">=</span>/work/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/job-array-example/<span class="si">${</span><span class="nv">SLURM_JOBID</span><span class="si">}</span>
mkdir -p <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

cp <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>/test.py <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># each job will see a different ${SLURM_ARRAY_TASK_ID}</span>
<span class="nb">echo</span> <span class="s2">&quot;now processing task id:: &quot;</span> <span class="si">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="si">}</span>
python test.py &gt; output_<span class="si">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="si">}</span>.txt

<span class="c1"># after the job is done we copy our output back to $SLURM_SUBMIT_DIR</span>
cp output_<span class="si">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="si">}</span>.txt <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>

<span class="c1"># we step out of the scratch directory and remove it</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>
rm -rf <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># Done</span>
<span class="nb">exit</span> <span class="m">0</span>
</pre></div>
</div>
<p>Submit the script and after a short while you should see 16 output files
in your submit directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ls -l output*.txt

-rw------- 1 user user 60 Oct 14 14:44 output_1.txt
-rw------- 1 user user 60 Oct 14 14:44 output_10.txt
-rw------- 1 user user 60 Oct 14 14:44 output_11.txt
-rw------- 1 user user 60 Oct 14 14:44 output_12.txt
-rw------- 1 user user 60 Oct 14 14:44 output_13.txt
-rw------- 1 user user 60 Oct 14 14:44 output_14.txt
-rw------- 1 user user 60 Oct 14 14:44 output_15.txt
-rw------- 1 user user 60 Oct 14 14:44 output_16.txt
-rw------- 1 user user 60 Oct 14 14:44 output_2.txt
-rw------- 1 user user 60 Oct 14 14:44 output_3.txt
-rw------- 1 user user 60 Oct 14 14:44 output_4.txt
-rw------- 1 user user 60 Oct 14 14:44 output_5.txt
-rw------- 1 user user 60 Oct 14 14:44 output_6.txt
-rw------- 1 user user 60 Oct 14 14:44 output_7.txt
-rw------- 1 user user 60 Oct 14 14:44 output_8.txt
-rw------- 1 user user 60 Oct 14 14:44 output_9.txt
</pre></div>
</div>
</section>
<section id="packaging-smaller-parallel-jobs-into-one-large-parallel-job">
<h3>Packaging smaller parallel jobs into one large parallel job<a class="headerlink" href="#packaging-smaller-parallel-jobs-into-one-large-parallel-job" title="Permalink to this headline">¶</a></h3>
<p>There are several ways to package smaller parallel jobs into one large parallel
job. The preferred way is to use Job Arrays. Browse the web for many examples
on how to do it. Here we want to present an alternative which
can give a lot of flexibility.</p>
<p>In this example we wish to run 5 MPI jobs at the same time, each using 4 tasks,
thus totalling to 20 tasks.  Once they finish, we wish to
do a post-processing step and then resubmit another set of 5 jobs with 4 tasks each:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=example</span>
<span class="c1">#SBATCH --ntasks=20</span>
<span class="c1">#SBATCH --time=0-00:05:00</span>
<span class="c1">#SBATCH --mem-per-cpu=500MB</span>

<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>

<span class="c1"># first set of parallel runs</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>

<span class="nb">wait</span>

<span class="c1"># here a post-processing step</span>
<span class="c1"># ...</span>

<span class="c1"># another set of parallel runs</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>
mpirun -n <span class="m">4</span> ./my-binary <span class="p">&amp;</span>

<span class="nb">wait</span>

<span class="nb">exit</span> <span class="m">0</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">wait</span></code> commands are important here - the run script will only continue
once all commands containing <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> have completed.</p>
</section>
<section id="example-on-how-to-allocate-entire-memory-on-one-node">
<span id="allocated-entire-memory"></span><h3>Example on how to allocate entire memory on one node<a class="headerlink" href="#example-on-how-to-allocate-entire-memory-on-one-node" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>

<span class="c1">###################################################</span>
<span class="c1"># Example for a job that consumes a lot of memory #</span>
<span class="c1">###################################################</span>

<span class="c1">#SBATCH --job-name=example</span>

<span class="c1"># we ask for 1 node</span>
<span class="c1">#SBATCH --nodes=1</span>

<span class="c1"># run for five minutes</span>
<span class="c1">#              d-hh:mm:ss</span>
<span class="c1">#SBATCH --time=0-00:05:00</span>

<span class="c1"># total memory for this job</span>
<span class="c1"># this is a hard limit</span>
<span class="c1"># note that if you ask for more than one CPU has, your account gets</span>
<span class="c1"># charged for the other (idle) CPUs as well</span>
<span class="c1">#SBATCH --mem=31000MB</span>

<span class="c1"># you can turn on all mail notification</span>
<span class="c1">#SBATCH --mail-type=ALL</span>

<span class="c1"># you may not place bash commands before the last SBATCH directive</span>

<span class="c1"># define and create a unique scratch directory</span>
<span class="nv">SCRATCH_DIRECTORY</span><span class="o">=</span>/work/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/example/<span class="si">${</span><span class="nv">SLURM_JOBID</span><span class="si">}</span>
mkdir -p <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we copy everything we need to the scratch directory</span>
<span class="c1"># ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from</span>
cp <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>/my_binary.x <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we execute the job and time it</span>
<span class="nb">time</span> ./my_binary.x &gt; my_output

<span class="c1"># after the job is done we copy our output back to $SLURM_SUBMIT_DIR</span>
cp <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>/my_output <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>

<span class="c1"># we step out of the scratch directory and remove it</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>
rm -rf <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># happy end</span>
<span class="nb">exit</span> <span class="m">0</span>
</pre></div>
</div>
</section>
<section id="how-to-recover-files-before-a-job-times-out">
<h3>How to recover files before a job times out<a class="headerlink" href="#how-to-recover-files-before-a-job-times-out" title="Permalink to this headline">¶</a></h3>
<p>Possibly you would like to clean up the work directory or recover
files for restart in case a job times out. In this example we ask Slurm
to send a signal to our script 120 seconds before it times out to give
us a chance to perform clean-up actions.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>

<span class="c1"># job name</span>
<span class="c1">#SBATCH --job-name=example</span>

<span class="c1"># replace this by your account</span>
<span class="c1">#SBATCH --account=...</span>

<span class="c1"># one core only</span>
<span class="c1">#SBATCH --ntasks=1</span>

<span class="c1"># we give this job 4 minutes</span>
<span class="c1">#SBATCH --time=0-00:04:00</span>

<span class="c1"># asks SLURM to send the USR1 signal 120 seconds before end of the time limit</span>
<span class="c1">#SBATCH --signal=B:USR1@120</span>

<span class="c1"># define the handler function</span>
<span class="c1"># note that this is not executed here, but rather</span>
<span class="c1"># when the associated signal is sent</span>
your_cleanup_function<span class="o">()</span>
<span class="o">{</span>
    <span class="nb">echo</span> <span class="s2">&quot;function your_cleanup_function called at </span><span class="k">$(</span>date<span class="k">)</span><span class="s2">&quot;</span>
    <span class="c1"># do whatever cleanup you want here</span>
<span class="o">}</span>

<span class="c1"># call your_cleanup_function once we receive USR1 signal</span>
<span class="nb">trap</span> <span class="s1">&#39;your_cleanup_function&#39;</span> USR1

<span class="nb">echo</span> <span class="s2">&quot;starting calculation at </span><span class="k">$(</span>date<span class="k">)</span><span class="s2">&quot;</span>

<span class="c1"># the calculation &quot;computes&quot; (in this case sleeps) for 1000 seconds</span>
<span class="c1"># but we asked slurm only for 240 seconds so it will not finish</span>
<span class="c1"># the &quot;&amp;&quot; after the compute step and &quot;wait&quot; are important</span>
sleep <span class="m">1000</span> <span class="p">&amp;</span>
<span class="nb">wait</span>
</pre></div>
</div>
</section>
</section>
<section id="openmp-and-mpi">
<h2>OpenMP and MPI<a class="headerlink" href="#openmp-and-mpi" title="Permalink to this headline">¶</a></h2>
<p>You can download the examples given here to a file (e.g. run.sh) and start it with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ sbatch run.sh
</pre></div>
</div>
<section id="example-for-an-openmp-job">
<h3>Example for an OpenMP job<a class="headerlink" href="#example-for-an-openmp-job" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>

<span class="c1">#############################</span>
<span class="c1"># example for an OpenMP job #</span>
<span class="c1">#############################</span>

<span class="c1">#SBATCH --job-name=example</span>

<span class="c1"># we ask for 1 task with 20 cores</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --cpus-per-task=20</span>

<span class="c1"># exclusive makes all memory available</span>
<span class="c1">#SBATCH --exclusive</span>

<span class="c1"># run for five minutes</span>
<span class="c1">#              d-hh:mm:ss</span>
<span class="c1">#SBATCH --time=0-00:05:00</span>

<span class="c1"># turn on all mail notification</span>
<span class="c1">#SBATCH --mail-type=ALL</span>

<span class="c1"># you may not place bash commands before the last SBATCH directive</span>

<span class="c1"># define and create a unique scratch directory</span>
<span class="nv">SCRATCH_DIRECTORY</span><span class="o">=</span>/global/work/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/example/<span class="si">${</span><span class="nv">SLURM_JOBID</span><span class="si">}</span>
mkdir -p <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we copy everything we need to the scratch directory</span>
<span class="c1"># ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from</span>
cp <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>/my_binary.x <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we set OMP_NUM_THREADS to the number of available cores</span>
<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span>

<span class="c1"># we execute the job and time it</span>
<span class="nb">time</span> ./my_binary.x &gt; my_output

<span class="c1"># after the job is done we copy our output back to $SLURM_SUBMIT_DIR</span>
cp <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>/my_output <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>

<span class="c1"># we step out of the scratch directory and remove it</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>
rm -rf <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># happy end</span>
<span class="nb">exit</span> <span class="m">0</span>
</pre></div>
</div>
</section>
<section id="example-for-a-mpi-job">
<h3>Example for a MPI job<a class="headerlink" href="#example-for-a-mpi-job" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>

<span class="c1">##########################</span>
<span class="c1"># example for an MPI job #</span>
<span class="c1">##########################</span>

<span class="c1">#SBATCH --job-name=example</span>

<span class="c1"># 80 MPI tasks in total</span>
<span class="c1"># Stallo has 16 or 20 cores/node and therefore we take</span>
<span class="c1"># a number that is divisible by both</span>
<span class="c1">#SBATCH --ntasks=80</span>

<span class="c1"># run for five minutes</span>
<span class="c1">#              d-hh:mm:ss</span>
<span class="c1">#SBATCH --time=0-00:05:00</span>

<span class="c1"># 500MB memory per core</span>
<span class="c1"># this is a hard limit</span>
<span class="c1">#SBATCH --mem-per-cpu=500MB</span>

<span class="c1"># turn on all mail notification</span>
<span class="c1">#SBATCH --mail-type=ALL</span>

<span class="c1"># you may not place bash commands before the last SBATCH directive</span>

<span class="c1"># define and create a unique scratch directory</span>
<span class="nv">SCRATCH_DIRECTORY</span><span class="o">=</span>/global/work/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/example/<span class="si">${</span><span class="nv">SLURM_JOBID</span><span class="si">}</span>
mkdir -p <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we copy everything we need to the scratch directory</span>
<span class="c1"># ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from</span>
cp <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>/my_binary.x <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we execute the job and time it</span>
<span class="nb">time</span> mpirun -np <span class="nv">$SLURM_NTASKS</span> ./my_binary.x &gt; my_output

<span class="c1"># after the job is done we copy our output back to $SLURM_SUBMIT_DIR</span>
cp <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>/my_output <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>

<span class="c1"># we step out of the scratch directory and remove it</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>
rm -rf <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># happy end</span>
<span class="nb">exit</span> <span class="m">0</span>
</pre></div>
</div>
</section>
<section id="example-for-a-hybrid-mpi-openmp-job">
<h3>Example for a hybrid MPI/OpenMP job<a class="headerlink" href="#example-for-a-hybrid-mpi-openmp-job" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>

<span class="c1">#######################################</span>
<span class="c1"># example for a hybrid MPI OpenMP job #</span>
<span class="c1">#######################################</span>

<span class="c1">#SBATCH --job-name=example</span>

<span class="c1"># we ask for 4 MPI tasks with 10 cores each</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --ntasks-per-node=2</span>
<span class="c1">#SBATCH --cpus-per-task=10</span>

<span class="c1"># run for five minutes</span>
<span class="c1">#              d-hh:mm:ss</span>
<span class="c1">#SBATCH --time=0-00:05:00</span>

<span class="c1"># 500MB memory per core</span>
<span class="c1"># this is a hard limit</span>
<span class="c1">#SBATCH --mem-per-cpu=500MB</span>

<span class="c1"># turn on all mail notification</span>
<span class="c1">#SBATCH --mail-type=ALL</span>

<span class="c1"># you may not place bash commands before the last SBATCH directive</span>

<span class="c1"># define and create a unique scratch directory</span>
<span class="nv">SCRATCH_DIRECTORY</span><span class="o">=</span>/global/work/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/example/<span class="si">${</span><span class="nv">SLURM_JOBID</span><span class="si">}</span>
mkdir -p <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we copy everything we need to the scratch directory</span>
<span class="c1"># ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from</span>
cp <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>/my_binary.x <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># we set OMP_NUM_THREADS to the number cpu cores per MPI task</span>
<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span>

<span class="c1"># we execute the job and time it</span>
<span class="nb">time</span> mpirun -np <span class="nv">$SLURM_NTASKS</span> ./my_binary.x &gt; my_output

<span class="c1"># after the job is done we copy our output back to $SLURM_SUBMIT_DIR</span>
cp <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>/my_output <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>

<span class="c1"># we step out of the scratch directory and remove it</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="si">}</span>
rm -rf <span class="si">${</span><span class="nv">SCRATCH_DIRECTORY</span><span class="si">}</span>

<span class="c1"># happy end</span>
<span class="nb">exit</span> <span class="m">0</span>
</pre></div>
</div>
<p>If you want to start more than one MPI rank per node you can
use <code class="docutils literal notranslate"><span class="pre">--ntasks-per-node</span></code> in combination with <code class="docutils literal notranslate"><span class="pre">--nodes</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=4 --ntasks-per-node=2 --cpus-per-task=8</span>
</pre></div>
</div>
<p>This will start 2 MPI tasks each on 4 nodes, where each task can use up to 8 threads.</p>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="slurm_parameter.html" class="btn btn-neutral float-right" title="SLURM Workload Manager" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="batch.html" class="btn btn-neutral float-left" title="Batch system" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, HPC Physics group - NTNU

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>